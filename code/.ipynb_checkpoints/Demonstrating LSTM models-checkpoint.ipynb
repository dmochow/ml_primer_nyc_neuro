{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c78917",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence modeling of time series\n",
    "* Recurrent neural nets are popular when the data has a temporal component\n",
    "* A popular sequence model is known as the \"Long Short Term Memory\" (LSTM) network\n",
    "    * LSTM models are able to retain information across long time scales in the input sequence\n",
    "* Here we will apply the LSTM to our EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86705a93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the data\n",
    "* The second and third dimensions are swapped to accommodate the upcoming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/EEG.mat' # the data file\n",
    "import scipy.io as sio # the library that we use to load Matlab data into Python\n",
    "data = sio.loadmat(filename)\n",
    "x_train = np.swapaxes(data['X_train'],1,2)\n",
    "x_test = np.swapaxes(data['X_test'],1,2)\n",
    "y_train = data['Y_train']\n",
    "y_test = data['Y_test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dd004",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the model\n",
    "* We will work with the Keras LSTM class \n",
    "* A key parameter is the number of hidden units in our LSTM layer - here we start with 100 units\n",
    "* We add a \"dropout\" parameter to try to mitigate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0da09b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "input_shape=(128,64) # this tells Keras the shape of the data that will be passed to it for training/testing\n",
    "n_classes=2\n",
    "n_lstm_units=20\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.LSTM(n_lstm_units,dropout=0.2),\n",
    "        keras.layers.Dense(n_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f0195",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compile the model to prepare for training\n",
    "* We will use the same training parameters as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # how many training examples in each \"batch\" used to compute gradient of loss function\n",
    "epochs = 30 # how many passes through the data we want the fitting to take\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceec1de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting the model\n",
    "* Note that the LSTM is a complex architecture and is thus heavily parameterized\n",
    "* We will pay close attention to possible overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c81041",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7da28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the sequence model\n",
    "* The training performance was quite high, but as always, we need to test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=model.evaluate(x_test,y_test)\n",
    "print(\"Model accuracy = \" + str(metrics[1]))\n",
    "print(\"Model AUROC = \" + str(metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb49f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting the performance\n",
    "* Note that while the performance is very good, it did not exceed that obtained with a convolutional model\n",
    "* It may be argued that the information in EEG is in the motifs, and not embedded in a true sequence (order-dependence)\n",
    "* In practice, the number of units and dropout parameter are extensively tuned on a \"validation\" set"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 427.847472,
   "position": {
    "height": "40px",
    "left": "1720px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
