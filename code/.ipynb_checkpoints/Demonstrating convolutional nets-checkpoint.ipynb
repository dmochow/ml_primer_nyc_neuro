{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986d34db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying visual perception from scalp EEG\n",
    "* We will work with a dataset where subjects observed images of target and non-target objects while having their neural activity recorded with scalp EEG\n",
    "* We will create and fit a model that predicts which category of object they saw based on their observed brain activity\n",
    "* This takes the form of a binary classification problem. \n",
    "* The model that we will use is known as a \"convolutional\" network, due to it transforming the input data in a manner analogous to filtering in linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab955d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data\n",
    "* The data is contained in a Matlab data file\n",
    "* We use the scipy package to conveniently bring it into the Python workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc654a6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "filename = '../data/EEG.mat' # the data file\n",
    "import scipy.io as sio # the library that we use to load Matlab data into Python\n",
    "data = sio.loadmat(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f285c07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspect the data\n",
    "* The data has been partitioned into a training and test set\n",
    "* Features are shaped (example, electrode, time samples)\n",
    "* Labels are shaped (example, 2), where the 2 refers to the two classes (target vs non-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41711b77",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x_train = data['X_train']\n",
    "x_test = data['X_test']\n",
    "y_train = data['Y_train']\n",
    "y_test = data['Y_test']\n",
    "\n",
    "print(\"The shape of the training features is \" + str(x_train.shape))\n",
    "print(\"The shape of the training labels is \" + str(y_train.shape))\n",
    "print(\"The shape of the test features is \" + str(x_test.shape))\n",
    "print(\"The shape of the test labels is \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ecc90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a sample trial\n",
    "* Looking at the data is always recommended\n",
    "* Here the data takes the form of space-time matrix, that we plot as overlaid waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c77cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Python's array library\n",
    "import matplotlib.pyplot as plt # a popular library for plotting in Python\n",
    "trial_idx = 500 # let's look at trial 501\n",
    "sample_eeg = np.squeeze(x_train[trial_idx,:,:])\n",
    "plt.plot(sample_eeg.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca985f91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspect the label information\n",
    "* It is common to label each example with a vector of probabilities\n",
    "* Below, the first element of each vector denotes P(non-target)\n",
    "* Notice above that all but one of the first 10 examples was a non-target. \n",
    "    * The 7th example was a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423bafb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y_train[0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cb3c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the model\n",
    "* We use the user-friendly Keras library to create a simple model for predicting target vs non-target based on the array of EEG values\n",
    "* The first layer of our model corresponds to the input data\n",
    "* The second layer performs a convolution (in effect spatial filtering) across the data in windows of 10 samples each\n",
    "* The final layer transforms the output of the convolution into a probability value\n",
    "* Our model has 881 parameters. We have 1058 examples. \n",
    "    * It's a good rule-of-thumb to have at least as many training examples as model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f61350",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "input_shape=(64,128,1) # this tells Keras the shape of the data that will be passed to it for training/testing\n",
    "num_classes=2\n",
    "num_filters = 1\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(num_filters, kernel_size=(64, 10), activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9a99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compile the model\n",
    "* We need to tell Keras what type of loss function we will be using, and which optimizer we would like. \n",
    "* The batch size controls how many examples are used for each computation of the gradient of the loss function\n",
    "* The number of epochs controls how many complete passes of the data we would like to make\n",
    "* We also tell Keras that we want to monitor both the model accuracy, as well as the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea6032",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # how many training examples in each \"batch\" used to compute gradient of loss function\n",
    "epochs = 10 # how many passes through the data we want the fitting to take\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd10bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting the model\n",
    "* Keras allows you to monitor the training performance during model fitting\n",
    "* Notice that the training performance is quite good\n",
    "    * Area under the ROC curve of is ~ 0.87\n",
    "    * Now let's see how well the model generalizes to unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453e671",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aefe53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the model on unseen data.\n",
    "* We use the test set to assess our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cdd58a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate(x_test,y_test)\n",
    "print(\"The model accuracy is \" + str(metrics[1])) # element 0 is loss, 1 is accuracy, 2 is auroc\n",
    "print(\"The model AUROC is \" + str(metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602c903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Verify performance manually\n",
    "* We can use the predict function to generate model probabilities for the examples in the test set\n",
    "* A probability > 0.5 should be interpreted as the model predicting a target image\n",
    "* In the \"pred_labels\" variable below, the first column is P(non-target), while the second is P(target).\n",
    "* In the \"true_labels\" variable, the binary value indicates the ground truh (0 for non-target, 1 for target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d5490",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_scores = model.predict(x_test)\n",
    "pred_labels = np.argmax(test_scores,1)\n",
    "true_labels = np.argmax(y_test,1)\n",
    "test_accuracy = np.mean(true_labels==pred_labels)\n",
    "\n",
    "print(\"Predicted probabilities:\")\n",
    "print(test_scores[0:10,:])\n",
    "\n",
    "print(\"Test set labels:\")\n",
    "print(pred_labels[0:10])\n",
    "\n",
    "\n",
    "print(\"Accuracy = \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc9c29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Increasing the model complexity\n",
    "* We can increase the number of convolutional \"filters\" in our learning model\n",
    "* We can think of each filter as looking for a certain pattern in the EEG\n",
    "* If more than one distinct pattern reflects perception of an oddball, this could improve detection\n",
    "* We now have 3 times as many parameters. \n",
    "    * This puts us at risk of overfitting, but we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0c18c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_filters = 3\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(num_filters, kernel_size=(64, 10), activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a8f2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compiling and fitting the new model\n",
    "* We fit the model again\n",
    "* Due to the increase in the number of model parameters, the training performance is guaranteed to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472707b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.AUC()])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812a7a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the new model on unseen data\n",
    "* We are now ready to examine the model's performance on the test set\n",
    "* Indeed, we see a marked improvement on the AUROC\n",
    "    * The increased model complexity did not hurt us in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331248b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate(x_test,y_test)\n",
    "print(\"The model accuracy is \" + str(metrics[1]))\n",
    "print(\"The model AUROC is \" + str(metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36a7c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding regularization\n",
    "* Keras makes it easy to add both L1 and L2 regularization\n",
    "* We will add both forms to our model\n",
    "* Here we assign fairly arbitrary values to the so-called \"hyperparameters\"\n",
    "    * In practice, these have to be extensively tuned on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360c733",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "reg_l1_l2 = keras.regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(num_filters, kernel_size=(64, 10), activation=\"relu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer = reg_l1_l2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18353b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compiling and fitting the model\n",
    "* Once again, we need to compile and fit the hopefully improved model\n",
    "* With regularization, we expect the training accuracy to suffer, but hopefully also lead to improved generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948ee03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.AUC()])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "metrics = model.evaluate(x_test,y_test)\n",
    "print(\"The model accuracy is \" + str(metrics[1]))\n",
    "print(\"The model AUROC is \" + str(metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a9f37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unequal class prevalence\n",
    "* Targets are infrequent in P300 experiments (by design)\n",
    "* This class imbalance may lead to the model fitting emphasizing the more frequent non-target class\n",
    "* First we need to measure the proportion of the two classes in the _training_ set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b42e43",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = np.argmax(y_train,1)\n",
    "n_non_targets = np.sum(train_labels==0)\n",
    "n_targets=np.sum(train_labels==1)\n",
    "class_ratio = n_targets/n_non_targets\n",
    "print(\"The ratio of targets to non targets is \" + str(class_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a646b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding class weighting\n",
    "* Now we add class weighting to balance the contribution of the classes to the loss function\n",
    "* This option is included in Keras fit function\n",
    "* After fitting the model with the class weights, we evaluate performance one last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23ef0c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class_weight={0:1,1:1/class_ratio}\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",keras.metrics.AUC()])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weight)\n",
    "metrics = model.evaluate(x_test,y_test)\n",
    "print(\"The model accuracy is \" + str(metrics[1]))\n",
    "print(\"The model AUROC is \" + str(metrics[2]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 648.38578,
   "position": {
    "height": "40px",
    "left": "1519.11px",
    "right": "20px",
    "top": "120px",
    "width": "484.774px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
